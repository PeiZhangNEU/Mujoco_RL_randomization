{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28a7a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if(torch.cuda.is_available()):\n",
    "    device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b3fa5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer_actor = SummaryWriter('PPO_logs/actor')\n",
    "writer_critic = SummaryWriter('PPO_logs/critic')\n",
    "writer_scalars = SummaryWriter('PPO_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e44f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "        \n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "808078e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    '''给出一个分布的均值'''\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.linear1 = nn.Linear(state_dim, 64)\n",
    "        self.linear2 = nn.Linear(64, 64)\n",
    "        self.linear3 = nn.Linear(64, action_dim)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = torch.tanh(self.linear1(state))\n",
    "        x = torch.tanh(self.linear2(x))\n",
    "        x = torch.tanh(self.linear3(x))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "697624e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.linear1 = nn.Linear(state_dim, 64)\n",
    "        self.linear2 = nn.Linear(64, 64)\n",
    "        self.linear3 = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = torch.tanh(self.linear1(state))\n",
    "        x = torch.tanh(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3a7c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    '''把Actor和Critic封装到一起了，这样可以直接评估，比较简单'''\n",
    "    def __init__(self, state_dim, action_dim, action_std_init):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.action_dim = action_dim\n",
    "        self.action_var = torch.full((self.action_dim,), action_std_init**2).to(device) # 把一个标量变成动作维度的向量\n",
    "        \n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.critic = Critic(state_dim)\n",
    "        \n",
    "    def set_action_std(self, new_action_std):\n",
    "        self.action_var = torch.full((self.action_dim,), new_action_std**2).to(device)\n",
    "        \n",
    "    def act(self, state):\n",
    "        '''输入单个状态，得到动作和log, 是之后要用到选择动作的子函数'''\n",
    "        action_mean = self.actor(state)           # 一行均值，列数是动作维度，1维向量\n",
    "        cov_mat = torch.diag(self.action_var)     # 一行方差，可以直接diag\n",
    "        dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        \n",
    "        return action.detach(), action_logprob.detach()  # [x,x,x], x\n",
    "    \n",
    "    def evaluate(self, state, action):\n",
    "        '''输入一批状态和动作，得到一批评估结果, .log_prob(action)中action不必是这个dist筛出来的'''\n",
    "        action_mean = self.actor(state)          # 好多行均值，列数是动作维度，2维矩阵\n",
    "        action_var = self.action_var.expand_as(action_mean)  # 方差也要变成好多行\n",
    "        cov_mat = torch.diag_embed(action_var)   # 不能直接diag，要用diag_embed\n",
    "        dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        \n",
    "        # 如果动作的dim是1，则需要给他改变一下形状，\n",
    "        # 因为多维action[batch_size, action_dim]，而单维的action[batch_size]，少了个维度,要变成[bactch_size,1]\n",
    "        if self.action_dim == 1:\n",
    "            action = action.reshape(-1, 1)   # [batch_size, 1] 只有第二个维度有，下面的log_prob才好使\n",
    "        \n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        state_values = self.critic(state)\n",
    "        dist_entropy = dist.entropy()\n",
    "        \n",
    "        return action_logprobs, state_values, dist_entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9616dbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, env, lr_actor, lr_critic, gamma, K_epochs, eps_clip, actions_std_init=0.6):\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "        self.max_action = float(env.action_space.high[0])\n",
    "        \n",
    "        self.action_std = actions_std_init\n",
    "        self.gamma = gamma\n",
    "        self.K_epochs = K_epochs\n",
    "        self.eps_clip = eps_clip\n",
    "        self.actions_std_init = actions_std_init\n",
    "        self.buffer = RolloutBuffer()\n",
    "        \n",
    "        # 初始化Actor_Critic 整体网络\n",
    "        self.policy = ActorCritic(self.state_dim, self.action_dim, self.action_std).to(device)\n",
    "        self.policy_old = ActorCritic(self.state_dim, self.action_dim, self.action_std).to(device)\n",
    "        \n",
    "        # 初始化优化器和损失函数\n",
    "        # 因为一个policy里面有两个网络，所以优化器的参数和学习率是两个字典组成的列表\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params':self.policy.actor.parameters(), 'lr':lr_actor},\n",
    "            {'params':self.policy.critic.parameters(), 'lr':lr_critic}\n",
    "        ])\n",
    "\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.summary = {}\n",
    "    \n",
    "    def set_action_std(self, new_action_std):\n",
    "        '''给ActorCritic类设置一下std'''\n",
    "        self.policy.set_action_std(new_action_std)\n",
    "        self.policy_old.set_action_std(new_action_std)\n",
    " \n",
    "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
    "        '''随着步数增大， 减小action的方差, 并给两个actor网络设置方差'''\n",
    "        self.action_std -= action_std_decay_rate\n",
    "        self.action_std = round(self.action_std, 4)      # 保留四位小数\n",
    "        if (self.action_std <= min_action_std):\n",
    "            self.action_std = min_action_std\n",
    "        self.set_action_std(self.action_std)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        '''根据状态获取动作，得到的动作是用于与环境交互的， 不用于update'''\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            action, action_log_prob = self.policy_old.act(state)\n",
    "        \n",
    "        # 选择动作的同时，把state， action, action_log_prob 加入buffer,都是tensor\n",
    "        self.buffer.states.append(state)\n",
    "        self.buffer.actions.append(action)    \n",
    "        self.buffer.logprobs.append(action_log_prob)\n",
    "        \n",
    "        return self.max_action * action.detach().cpu().numpy()\n",
    "    \n",
    "    def test_select_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            action = self.policy_old.actor(state)\n",
    "            return self.max_action * action.detach().numpy()\n",
    "         \n",
    "    def update(self):\n",
    "        # 先把reward转换为能用的折扣过的rewards\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)  # 把折扣过的动作重头排列\n",
    "        # 归一化reward\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        rewards = (rewards - rewards.mean())/(rewards.std() + 1e-7)  # [4000]\n",
    "        \n",
    "        # buffer里面的这三个东西本来就是tensor，所以纵向连接dim=0也要用tensor的连接方法\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)   # [4000, 11]\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device) # [4000, 3]\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device) # [4000]\n",
    "        \n",
    "        # update\n",
    "        for _ in range(self.K_epochs):\n",
    "            \n",
    "            # 用现在的actor和critic，评估旧的states和actions\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "        #   [4000],   [4000, 1]     [4000]\n",
    "            \n",
    "            # 把state_values 变成[batch_size] 和rewards形状一致\n",
    "            state_values = torch.squeeze(state_values)\n",
    "            \n",
    "            # 计算ratio\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # 计算loss\n",
    "            advantages = rewards - state_values.detach()\n",
    "            surr1 = ratios *  advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            \n",
    "            # PPO 的最终loss\n",
    "            loss = -torch.min(surr1, surr2) - 0.01 * dist_entropy + 0.5 * self.loss_fn(state_values, rewards) #[4000]\n",
    "            loss = loss.mean()\n",
    "            self.summary['loss'] = loss.item()\n",
    "            \n",
    "            # 梯度更新\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        # 更新目标网络，用load模型参数的方法来硬更新模型\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        # 清空buffer\n",
    "        self.buffer.clear()        \n",
    "        \n",
    "    \n",
    "    def save(self, checkpoint_path):\n",
    "        '''只保存模型参数'''\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "        \n",
    "    def load(self, checkpoint_path):\n",
    "        self.policy_old.load_state_dict(torch.load(checkpoint_path))\n",
    "        self.policy.load_state_dict(torch.load(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb6b51e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"InvertedPendulum-v2\"\n",
    "\n",
    "max_ep_len = 1000                # 每个回合最大步数\n",
    "max_train_timesteps = int(3e6)\n",
    "action_std = 0.6\n",
    "action_std_decay_rate = 0.05\n",
    "min_action_std = 0.1\n",
    "action_std_decay_freq = int(2.5e5)  # 多少步减小一下std\n",
    "print_freq = max_ep_len * 10        # 多少步打印一下奖励\n",
    "save_model_freq = int(1e5)          # 多少步保存一下模型\n",
    "\n",
    "update_timestep = max_ep_len*4  # buffer存4000组数据训练一次\n",
    "K_epochs = 80                   # update一次更新网络80次\n",
    "\n",
    "eps_clip = 0.2\n",
    "gamma = 0.99\n",
    "lr_actor = 0.0003\n",
    "lr_critic = 0.001\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "ppo_agent = PPO(env, lr_actor, lr_critic, gamma, K_epochs, eps_clip, action_std)\n",
    "\n",
    "directory = 'PPO_weigths' + '/' + env_name + '/'\n",
    "if not os.path.exists(directory):\n",
    "      os.makedirs(directory)\n",
    "checkpoint_path = directory + 'ppo_{}.pth'.format(env_name)  # 模型保存会覆盖这个文件，整套训练下来，就保存一个文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27c937f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_actor.add_graph(ppo_agent.policy.actor, torch.FloatTensor(env.reset()).unsqueeze(0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "685767d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_critic.add_graph(ppo_agent.policy.critic, torch.FloatTensor(env.reset()).unsqueeze(0).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81141f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating window glfw\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8136/1173094694.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_ep_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mppo_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 已经存入了三个东西到buffer了！\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 直接更新下一个状态到当前状态，没有出现next_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8136/4216466101.py\u001b[0m in \u001b[0;36mselect_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_log_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_old\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# 选择动作的同时，把state， action, action_log_prob 加入buffer,都是tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8136/3049519052.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0maction_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m           \u001b[1;31m# 一行均值，列数是动作维度，1维向量\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mcov_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_var\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# 一行方差，可以直接diag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultivariateNormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcov_mat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0maction_logprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\torch\\lib\\site-packages\\torch\\distributions\\multivariate_normal.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[0;32m    134\u001b[0m                                  \"with optional leading batch dimensions\")\n\u001b[0;32m    135\u001b[0m             \u001b[0mbatch_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_shapes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcovariance_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcovariance_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcovariance_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_shape\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mprecision_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_step = 0\n",
    "i_episod = 0\n",
    "\n",
    "# printing and logging variables\n",
    "print_running_reward = 0\n",
    "print_running_episodes = 0\n",
    "\n",
    "while time_step <= max_train_timesteps:  # 以最大步数作为终止条件，同之前的第二种训练方法\n",
    "    state = env.reset()\n",
    "    current_ep_reward = 0\n",
    "    ep_step = 0\n",
    "    for t in range(max_ep_len):\n",
    "#         env.render()\n",
    "        action = ppo_agent.select_action(state)  # 已经存入了三个东西到buffer了！\n",
    "        state, reward, done, _ = env.step(action) # 直接更新下一个状态到当前状态，没有出现next_state\n",
    "        \n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "        \n",
    "        time_step += 1\n",
    "        current_ep_reward += reward\n",
    "        ep_step += 1\n",
    "        \n",
    "        if time_step % update_timestep == 0:\n",
    "            ppo_agent.update()\n",
    "            writer_scalars.add_scalar('loss', ppo_agent.summary['loss'], time_step)\n",
    "        \n",
    "        if time_step % action_std_decay_freq == 0:\n",
    "            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "            \n",
    "        # printing average reward\n",
    "        if time_step % print_freq == 0:\n",
    "            # 打印这么些回合的平均回合奖励\n",
    "            print_avg_reward = print_running_reward / print_running_episodes\n",
    "            print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episod, time_step, print_avg_reward))\n",
    "\n",
    "            print_running_reward = 0\n",
    "            print_running_episodes = 0\n",
    "        \n",
    "        if time_step % save_model_freq == 0:\n",
    "            ppo_agent.save(checkpoint_path)\n",
    "            \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    writer_scalars.add_scalar('episode_reward', current_ep_reward, i_episod)\n",
    "    print_running_reward += current_ep_reward\n",
    "    print_running_episodes += 1\n",
    "    i_episod += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e66f592",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1b0ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_agent_new = PPO(env, lr_actor, lr_critic, gamma, K_epochs, eps_clip, action_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b03326d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_agent_new.load(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47443d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating window glfw\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11032/83824454.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mppo_agent_new\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_select_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\torch\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"human\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\torch\\lib\\site-packages\\gym\\envs\\mujoco\\mujoco_env.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, width, height, camera_id, camera_name)\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"human\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_viewer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\torch\\lib\\site-packages\\mujoco_py\\mjviewer.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loop_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loop_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m                 \u001b[0mrender_inner_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loop_count\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;31m# Markers and overlay are regenerated in every pass.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\torch\\lib\\site-packages\\mujoco_py\\mjviewer.py\u001b[0m in \u001b[0;36mrender_inner_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    180\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_overlay\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_full_overlay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m             \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_video\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                 \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_pixels_as_in_window\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\torch\\lib\\site-packages\\mujoco_py\\mjviewer.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gui_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m             \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mglfw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmjrendercontext.pyx\u001b[0m in \u001b[0;36mmujoco_py.cymj.MjRenderContextWindow.render\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\torch\\lib\\site-packages\\glfw\\__init__.py\u001b[0m in \u001b[0;36mswap_buffers\u001b[1;34m(window)\u001b[0m\n\u001b[0;32m   2285\u001b[0m         \u001b[0mvoid\u001b[0m \u001b[0mglfwSwapBuffers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGLFWwindow\u001b[0m\u001b[1;33m*\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2286\u001b[0m     \"\"\"\n\u001b[1;32m-> 2287\u001b[1;33m     \u001b[0m_glfw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglfwSwapBuffers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2289\u001b[0m \u001b[0m_glfw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglfwSwapInterval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\torch\\lib\\site-packages\\glfw\\__init__.py\u001b[0m in \u001b[0;36merrcheck\u001b[1;34m(result, *args)\u001b[0m\n\u001b[0;32m    625\u001b[0m     \u001b[0musing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0m_callback_exception_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m     \"\"\"\n\u001b[1;32m--> 627\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0merrcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m         \u001b[1;32mglobal\u001b[0m \u001b[0m_exc_info_from_callback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_exc_info_from_callback\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for j in range(100):\n",
    "    state = env.reset()\n",
    "    for i in range(100000):\n",
    "        env.render()\n",
    "        action = ppo_agent_new.test_select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81179e31",
   "metadata": {},
   "source": [
    "# discounted 的rewards求解过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08a3a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_rewards = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "is_terminals =     [False, False, False, True, False, False, True, False, False, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cc72eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.9\n",
      "2.71\n",
      "1.0\n",
      "1.9\n",
      "2.71\n",
      "1.0\n",
      "1.9\n",
      "2.71\n",
      "3.439\n"
     ]
    }
   ],
   "source": [
    "rewards = []\n",
    "discounted_reward = 0\n",
    "for reward, is_terminal in zip(reversed(original_rewards), reversed(is_terminals)):\n",
    "    if is_terminal:\n",
    "        discounted_reward = 0\n",
    "    discounted_reward = reward + (0.9 * discounted_reward)\n",
    "    print(discounted_reward)\n",
    "    rewards.insert(0, discounted_reward)  # 把折扣过的动作倒叙排列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "304c7e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.439, 2.71, 1.9, 1.0, 2.71, 1.9, 1.0, 2.71, 1.9, 1.0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a822b5",
   "metadata": {},
   "source": [
    "# 单维Multivariate分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5e7ef85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.5000, 0.6000])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_mean = torch.FloatTensor([0.1, 0.5, 0.6])\n",
    "action_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "680bfa7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = torch.full((3,), 1.)\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1b166897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag = torch.diag(var)\n",
    "diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a28d11a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_single = MultivariateNormal(action_mean, diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4f8673c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2345, -1.1521,  0.3124])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = dist_single.sample()\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "cdbcfcc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4.1720)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_single.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974c2447",
   "metadata": {},
   "source": [
    "# 多维Multivariate分布\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aca735",
   "metadata": {},
   "source": [
    "## 动作是多维的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fafbd985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.5000, 0.6000],\n",
       "        [0.1000, 0.5000, 0.6000],\n",
       "        [0.1000, 0.5000, 0.6000],\n",
       "        [0.1000, 0.5000, 0.6000],\n",
       "        [0.1000, 0.5000, 0.6000]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_mean = [action_mean, action_mean, action_mean, action_mean, action_mean]\n",
    "actions_mean_tensor = torch.stack(actions_mean, dim=0)  # dim=0, 纵向拼接列表里面的元素\n",
    "actions_mean_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f9710e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varss = var.expand_as(actions_mean_tensor)\n",
    "varss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b4c96c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.]]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diag_emd = torch.diag_embed(varss)\n",
    "diag_emd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1aeeeed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_multi = MultivariateNormal(actions_mean_tensor, diag_emd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7ebfaab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2881,  0.5815, -0.3580],\n",
       "        [-0.7267,  0.3107,  1.7739],\n",
       "        [ 0.1329, -0.2787, -1.6822],\n",
       "        [ 0.9439,  1.5735,  0.6496],\n",
       "        [ 0.2225,  0.8475,  0.0482]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_multi.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "97cbf231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.7568, -2.7568, -2.7568, -2.7568, -2.7568])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_multi.log_prob(actions_mean_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ede0753f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.2568, 4.2568, 4.2568, 4.2568, 4.2568])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_multi.entropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690b6712",
   "metadata": {},
   "source": [
    "# stack函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04d0c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Hopper-v2')\n",
    "state = torch.FloatTensor(env.reset())\n",
    "states = [state, state, state, state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0098fc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.2547e+00,  2.4694e-03, -7.5729e-04,  3.7093e-03, -4.7297e-03,\n",
      "        -2.5605e-03,  7.4058e-04,  9.3807e-04, -1.6994e-03, -1.6434e-03,\n",
      "         4.7490e-03])\n",
      "torch.Size([11])\n"
     ]
    }
   ],
   "source": [
    "print(state)\n",
    "print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c09250f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 1.2547e+00,  2.4694e-03, -7.5729e-04,  3.7093e-03, -4.7297e-03,\n",
       "         -2.5605e-03,  7.4058e-04,  9.3807e-04, -1.6994e-03, -1.6434e-03,\n",
       "          4.7490e-03]),\n",
       " tensor([ 1.2547e+00,  2.4694e-03, -7.5729e-04,  3.7093e-03, -4.7297e-03,\n",
       "         -2.5605e-03,  7.4058e-04,  9.3807e-04, -1.6994e-03, -1.6434e-03,\n",
       "          4.7490e-03]),\n",
       " tensor([ 1.2547e+00,  2.4694e-03, -7.5729e-04,  3.7093e-03, -4.7297e-03,\n",
       "         -2.5605e-03,  7.4058e-04,  9.3807e-04, -1.6994e-03, -1.6434e-03,\n",
       "          4.7490e-03]),\n",
       " tensor([ 1.2547e+00,  2.4694e-03, -7.5729e-04,  3.7093e-03, -4.7297e-03,\n",
       "         -2.5605e-03,  7.4058e-04,  9.3807e-04, -1.6994e-03, -1.6434e-03,\n",
       "          4.7490e-03])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2592c704",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_states = torch.stack(states, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cf31da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 11])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7793380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 11])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.squeeze(old_states).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c17a5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob = torch.tensor(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07c1df70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b38bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = [log_prob, log_prob, log_prob, log_prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f3ed8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(log_probs, dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d620cc82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "188955bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque  # 双向队列\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import torch.nn.functional as F # 这里面有onehot函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d32b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6dea22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_policy_graph = SummaryWriter('sac_2018_actions/policy_net')\n",
    "writer_softq_graph = SummaryWriter('sac_2018_actions/SoftQ_net')\n",
    "writer_value_graph = SummaryWriter('sac_2018_actions/value_net')\n",
    "writer_scale = SummaryWriter('sac_2018_actions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36d1c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    '''评估状态价值'''\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.l1 = nn.Linear(input_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, output_dim)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.l1(state))\n",
    "        x = torch.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "\n",
    "class SoftQNetwork(nn.Module):\n",
    "    '''评估离散动作价值'''\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=256):\n",
    "        super(SoftQNetwork, self).__init__()\n",
    "        self.l1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, num_actions)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.l1(state))\n",
    "        x = torch.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "    \n",
    "class PolicyNetwork(nn.Module):\n",
    "    '''Actor输出多个离散动作的probs'''\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=256):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.l1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, num_actions)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        '''根据给定的state得到各个离散动作的probs'''\n",
    "        x = torch.relu(self.l1(state))\n",
    "        x = torch.relu(self.l2(x))\n",
    "        x = torch.softmax(self.l3(x), dim=-1)\n",
    "        return x\n",
    "    \n",
    "    def sample(self, state):\n",
    "        '''根据state得到probs，从而得到action和logporb'''\n",
    "        action_probs = self.forward(state)\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        \n",
    "        return action, action_logprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17201792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        experience = (state, action, np.array([reward]), next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        done_batch = []\n",
    "        \n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        for experience in batch:\n",
    "            state, action, reward, next_state, done = experience\n",
    "            state_batch.append(state)\n",
    "            action_batch.append(action)\n",
    "            reward_batch.append(reward)\n",
    "            next_state_batch.append(next_state)\n",
    "            done_batch.append(done)\n",
    "            \n",
    "        return (state_batch, action_batch, reward_batch, next_state_batch, done_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "136cb519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(model, target_model, tau):\n",
    "    for target_pam, pam in zip(target_model.parameters(), model.parameters()):\n",
    "        target_pam.data.copy_((1. - tau) * target_pam + tau * pam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65d3e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC_Agent:\n",
    "    def __init__(self, env, gamma, tau, v_lr, q_lr, policy_lr, buffer_maxlen):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.env = env\n",
    "        self.obs_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.update_step = 0\n",
    "        self.delay_step = 2\n",
    "        \n",
    "        ## 初始化网络\n",
    "        self.policy_net = PolicyNetwork(self.obs_dim, self.action_dim).to(self.device)\n",
    "        self.q_net1 = SoftQNetwork(self.obs_dim, self.action_dim).to(self.device)\n",
    "        self.q_net2 = SoftQNetwork(self.obs_dim, self.action_dim).to(self.device)\n",
    "        self.value_net = ValueNetwork(self.obs_dim, 1).to(self.device)\n",
    "        self.target_value_net =  ValueNetwork(self.obs_dim, 1).to(self.device)\n",
    "        \n",
    "        # 初始化目标网络的权重\n",
    "        update_target(self.value_net, self.target_value_net, tau=1.)\n",
    "        \n",
    "        ## 优化器\n",
    "        self.q1_optimizer = optim.Adam(self.q_net1.parameters(), lr=q_lr)\n",
    "        self.q2_optimizer = optim.Adam(self.q_net2.parameters(), lr=q_lr)\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=policy_lr)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=v_lr)\n",
    "        \n",
    "        ## Q网络和V网络的损失函数，都是mse\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "        # 经验池\n",
    "        self.replay_buffer = BasicBuffer(buffer_maxlen)\n",
    "        \n",
    "        # 用来记录各种标量的字典\n",
    "        self.summuries = {}\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        '''根据probs选动作，就算概率小也能选到'''\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(self.device)\n",
    "            action,_ = self.policy_net.sample(state)\n",
    "        return action.detach().cpu().numpy().item()  # 返回0维数组的整形数字\n",
    "    \n",
    "    def test_get_action(self, state):\n",
    "        '''直接根据最大概率选动作，而不是小概率也有可能选中，测试时候使用'''\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(self.device)\n",
    "            action_prob = self.policy_net(state).detach().numpy()\n",
    "            action = np.argmax(action_prob)  # 返回整形动作驱动环境\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def update(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        \n",
    "        actions = torch.tensor(actions).to(self.device)   # 动作是index长整型值，需要tensor化\n",
    "        \n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)  # 单个的Bool值不能转换，但是列表可以\n",
    "        dones = dones.view(dones.size(0), -1)  # (batch_size, 1) 的形状\n",
    "        \n",
    "        # 计算q网络相关的值\n",
    "        v_actions, v_log_pi = self.policy_net.sample(states)    # [batch_size],[batch_size]\n",
    "        v_actions_onehot = F.one_hot(v_actions, self.action_dim)# [batch_size, action_dim]\n",
    "\n",
    "        v_log_pi = v_log_pi.view(-1, 1)                         # [batch_size,1]\n",
    "        \n",
    "        v_q1 = self.q_net1(states) * v_actions_onehot           # [batch_size, action_dim]\n",
    "        v_q1 = torch.sum(v_q1, -1, True)                       # [batch_size, 1]在倒数第一维度相加，保持维度\n",
    "        v_q2 = self.q_net2(states) * v_actions_onehot\n",
    "        v_q2 = torch.sum(v_q2, -1, True) \n",
    "        next_v = self.target_value_net(next_states)\n",
    "        \n",
    "        # value loss \n",
    "        v_target = torch.min(v_q1, v_q2) - v_log_pi.exp()       # [bacth_size, 1]\n",
    "        curr_v = self.value_net(states)                   # [bacth_size, 1]\n",
    "\n",
    "        v_loss = self.loss_fn(curr_v, v_target.detach()) # 目标值统统detach中断传播\n",
    "        \n",
    "        # q_loss and update_qnet\n",
    "        actions_onehot = F.one_hot(actions, self.action_dim)\n",
    "        \n",
    "        curr_q1 = self.q_net1(states) * actions_onehot\n",
    "        curr_q1 = torch.sum(curr_q1, -1, True)\n",
    "        curr_q2 = self.q_net2(states) * actions_onehot\n",
    "        curr_q2 = torch.sum(curr_q2, -1, True)\n",
    "        expected_q = rewards + (1 - dones) * self.gamma * next_v\n",
    "        q1_loss = self.loss_fn(curr_q1, expected_q.detach()) # 目标值不需要梯度计算，所以detach终止梯度\n",
    "        q2_loss = self.loss_fn(curr_q2, expected_q.detach())\n",
    "        self.summuries['q1_loss'] = q1_loss.detach().item()\n",
    "        self.summuries['q2_loss'] = q2_loss.detach().item()\n",
    "        \n",
    "        self.value_optimizer.zero_grad()\n",
    "        self.q1_optimizer.zero_grad()\n",
    "        self.q2_optimizer.zero_grad()\n",
    "        v_loss.backward()\n",
    "        q1_loss.backward()\n",
    "        q2_loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "        self.q1_optimizer.step()\n",
    "        self.q2_optimizer.step()\n",
    "        \n",
    "        # 延迟更新policy网络以及q目标网络，用的actions 是 (-1,1)之间的动作值\n",
    "        new_actions, log_pi = self.policy_net.sample(states)         # [batch_size],[batch_size]\n",
    "        new_actions_onehot = F.one_hot(new_actions, self.action_dim) # [batch_size],[batch_size]\n",
    "        log_pi = log_pi.view(-1, 1)                                  # [batch_size,1]\n",
    "        \n",
    "        if self.update_step % self.delay_step == 0:\n",
    "            # 更新 policy网络\n",
    "            min_q = torch.min(torch.sum(self.q_net1(states) * new_actions_onehot, -1, True), \n",
    "                              torch.sum(self.q_net2(states) * new_actions_onehot, -1, True))\n",
    "            \n",
    "            policy_loss = (log_pi.exp() - min_q).mean()\n",
    "            self.summuries['policy_loss'] = policy_loss.detach().item()\n",
    "            \n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.policy_optimizer.step()\n",
    "            \n",
    "            # 更新目标v网络\n",
    "            update_target(self.value_net, self.target_value_net, tau=self.tau)\n",
    "        \n",
    "        self.update_step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1701e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, max_episode, max_steps, batch_size, render=True):\n",
    "    global_step = 0\n",
    "    \n",
    "    for episode in range(max_episode):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_step = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "            episode_reward += reward  # 每个回合能获得多少奖励\n",
    "            global_step += 1          # 全部步数\n",
    "            episode_step += 1         # 每个回合能走几步\n",
    "            writer_scale.add_scalar('Main/every_step_reward', reward, global_step) # 每一小步的单步奖励\n",
    "            \n",
    "            if len(agent.replay_buffer.buffer) > batch_size:\n",
    "                agent.update(batch_size)\n",
    "                writer_scale.add_scalar('Loss/q1_loss', agent.summuries['q1_loss'], global_step)\n",
    "                writer_scale.add_scalar('Loss/q2_loss', agent.summuries['q1_loss'], global_step)\n",
    "                writer_scale.add_scalar('Loss/policy_loss',agent.summuries['policy_loss'], global_step)\n",
    "\n",
    "            if done or step == max_steps - 1:\n",
    "                writer_scale.add_scalar('Episode/episode_steps', episode_step, episode)\n",
    "                writer_scale.add_scalar('Episode/episode_rewards', episode_reward, episode)\n",
    "                print('Episode is {}, episod_reward is {}'.format(episode, episode_reward))\n",
    "                break\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0fc6d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('Ant-v2')\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff6d238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAC 2018 Params\n",
    "tau = 0.1\n",
    "gamma = 0.99\n",
    "value_lr = 3e-4\n",
    "q_lr = 3e-4\n",
    "policy_lr = 3e-4\n",
    "buffer_maxlen = 100000\n",
    "\n",
    "#2018 agent\n",
    "agent = SAC_Agent(env, gamma, tau, value_lr, q_lr, policy_lr, buffer_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "125f4efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_state_raw = env.reset()\n",
    "test_state = torch.FloatTensor(test_state_raw).unsqueeze(0).to(agent.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69a3e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_policy_graph.add_graph(agent.policy_net, test_state)\n",
    "writer_softq_graph.add_graph(agent.q_net1, test_state)\n",
    "writer_value_graph.add_graph(agent.value_net, test_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3acef0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode is 0, episod_reward is 19.0\n",
      "Episode is 1, episod_reward is 26.0\n",
      "Episode is 2, episod_reward is 20.0\n",
      "Episode is 3, episod_reward is 35.0\n",
      "Episode is 4, episod_reward is 12.0\n",
      "Episode is 5, episod_reward is 18.0\n",
      "Episode is 6, episod_reward is 38.0\n",
      "Episode is 7, episod_reward is 26.0\n",
      "Episode is 8, episod_reward is 31.0\n",
      "Episode is 9, episod_reward is 17.0\n",
      "Episode is 10, episod_reward is 13.0\n",
      "Episode is 11, episod_reward is 11.0\n",
      "Episode is 12, episod_reward is 22.0\n",
      "Episode is 13, episod_reward is 12.0\n",
      "Episode is 14, episod_reward is 10.0\n",
      "Episode is 15, episod_reward is 31.0\n",
      "Episode is 16, episod_reward is 20.0\n",
      "Episode is 17, episod_reward is 18.0\n",
      "Episode is 18, episod_reward is 13.0\n",
      "Episode is 19, episod_reward is 79.0\n",
      "Episode is 20, episod_reward is 19.0\n",
      "Episode is 21, episod_reward is 16.0\n",
      "Episode is 22, episod_reward is 27.0\n",
      "Episode is 23, episod_reward is 31.0\n",
      "Episode is 24, episod_reward is 21.0\n",
      "Episode is 25, episod_reward is 32.0\n",
      "Episode is 26, episod_reward is 10.0\n",
      "Episode is 27, episod_reward is 39.0\n",
      "Episode is 28, episod_reward is 14.0\n",
      "Episode is 29, episod_reward is 14.0\n",
      "Episode is 30, episod_reward is 31.0\n",
      "Episode is 31, episod_reward is 18.0\n",
      "Episode is 32, episod_reward is 25.0\n",
      "Episode is 33, episod_reward is 12.0\n",
      "Episode is 34, episod_reward is 37.0\n",
      "Episode is 35, episod_reward is 36.0\n",
      "Episode is 36, episod_reward is 16.0\n",
      "Episode is 37, episod_reward is 9.0\n",
      "Episode is 38, episod_reward is 42.0\n",
      "Episode is 39, episod_reward is 23.0\n",
      "Episode is 40, episod_reward is 26.0\n",
      "Episode is 41, episod_reward is 16.0\n",
      "Episode is 42, episod_reward is 15.0\n",
      "Episode is 43, episod_reward is 18.0\n",
      "Episode is 44, episod_reward is 29.0\n",
      "Episode is 45, episod_reward is 13.0\n",
      "Episode is 46, episod_reward is 11.0\n",
      "Episode is 47, episod_reward is 30.0\n",
      "Episode is 48, episod_reward is 15.0\n",
      "Episode is 49, episod_reward is 11.0\n",
      "Episode is 50, episod_reward is 14.0\n",
      "Episode is 51, episod_reward is 15.0\n",
      "Episode is 52, episod_reward is 11.0\n",
      "Episode is 53, episod_reward is 11.0\n",
      "Episode is 54, episod_reward is 31.0\n",
      "Episode is 55, episod_reward is 20.0\n",
      "Episode is 56, episod_reward is 14.0\n",
      "Episode is 57, episod_reward is 12.0\n",
      "Episode is 58, episod_reward is 10.0\n",
      "Episode is 59, episod_reward is 10.0\n",
      "Episode is 60, episod_reward is 37.0\n",
      "Episode is 61, episod_reward is 22.0\n",
      "Episode is 62, episod_reward is 22.0\n",
      "Episode is 63, episod_reward is 11.0\n",
      "Episode is 64, episod_reward is 18.0\n",
      "Episode is 65, episod_reward is 12.0\n",
      "Episode is 66, episod_reward is 19.0\n",
      "Episode is 67, episod_reward is 35.0\n",
      "Episode is 68, episod_reward is 12.0\n",
      "Episode is 69, episod_reward is 12.0\n",
      "Episode is 70, episod_reward is 13.0\n",
      "Episode is 71, episod_reward is 12.0\n",
      "Episode is 72, episod_reward is 21.0\n",
      "Episode is 73, episod_reward is 37.0\n",
      "Episode is 74, episod_reward is 13.0\n",
      "Episode is 75, episod_reward is 47.0\n",
      "Episode is 76, episod_reward is 12.0\n",
      "Episode is 77, episod_reward is 31.0\n",
      "Episode is 78, episod_reward is 21.0\n",
      "Episode is 79, episod_reward is 39.0\n",
      "Episode is 80, episod_reward is 14.0\n",
      "Episode is 81, episod_reward is 32.0\n",
      "Episode is 82, episod_reward is 39.0\n",
      "Episode is 83, episod_reward is 14.0\n",
      "Episode is 84, episod_reward is 13.0\n",
      "Episode is 85, episod_reward is 18.0\n",
      "Episode is 86, episod_reward is 62.0\n",
      "Episode is 87, episod_reward is 14.0\n",
      "Episode is 88, episod_reward is 18.0\n",
      "Episode is 89, episod_reward is 13.0\n",
      "Episode is 90, episod_reward is 33.0\n",
      "Episode is 91, episod_reward is 18.0\n",
      "Episode is 92, episod_reward is 48.0\n",
      "Episode is 93, episod_reward is 32.0\n",
      "Episode is 94, episod_reward is 16.0\n",
      "Episode is 95, episod_reward is 31.0\n",
      "Episode is 96, episod_reward is 12.0\n",
      "Episode is 97, episod_reward is 20.0\n",
      "Episode is 98, episod_reward is 12.0\n",
      "Episode is 99, episod_reward is 17.0\n",
      "Episode is 100, episod_reward is 23.0\n",
      "Episode is 101, episod_reward is 15.0\n",
      "Episode is 102, episod_reward is 14.0\n",
      "Episode is 103, episod_reward is 53.0\n",
      "Episode is 104, episod_reward is 8.0\n",
      "Episode is 105, episod_reward is 14.0\n",
      "Episode is 106, episod_reward is 29.0\n",
      "Episode is 107, episod_reward is 12.0\n",
      "Episode is 108, episod_reward is 23.0\n",
      "Episode is 109, episod_reward is 10.0\n",
      "Episode is 110, episod_reward is 9.0\n",
      "Episode is 111, episod_reward is 27.0\n",
      "Episode is 112, episod_reward is 10.0\n",
      "Episode is 113, episod_reward is 34.0\n",
      "Episode is 114, episod_reward is 46.0\n",
      "Episode is 115, episod_reward is 15.0\n",
      "Episode is 116, episod_reward is 42.0\n",
      "Episode is 117, episod_reward is 19.0\n",
      "Episode is 118, episod_reward is 32.0\n",
      "Episode is 119, episod_reward is 13.0\n",
      "Episode is 120, episod_reward is 26.0\n",
      "Episode is 121, episod_reward is 12.0\n",
      "Episode is 122, episod_reward is 48.0\n",
      "Episode is 123, episod_reward is 52.0\n",
      "Episode is 124, episod_reward is 24.0\n",
      "Episode is 125, episod_reward is 24.0\n",
      "Episode is 126, episod_reward is 17.0\n",
      "Episode is 127, episod_reward is 15.0\n",
      "Episode is 128, episod_reward is 23.0\n",
      "Episode is 129, episod_reward is 15.0\n",
      "Episode is 130, episod_reward is 36.0\n",
      "Episode is 131, episod_reward is 11.0\n",
      "Episode is 132, episod_reward is 20.0\n",
      "Episode is 133, episod_reward is 17.0\n",
      "Episode is 134, episod_reward is 42.0\n",
      "Episode is 135, episod_reward is 19.0\n",
      "Episode is 136, episod_reward is 26.0\n",
      "Episode is 137, episod_reward is 14.0\n",
      "Episode is 138, episod_reward is 21.0\n",
      "Episode is 139, episod_reward is 11.0\n",
      "Episode is 140, episod_reward is 28.0\n",
      "Episode is 141, episod_reward is 25.0\n",
      "Episode is 142, episod_reward is 22.0\n",
      "Episode is 143, episod_reward is 45.0\n",
      "Episode is 144, episod_reward is 20.0\n",
      "Episode is 145, episod_reward is 17.0\n",
      "Episode is 146, episod_reward is 10.0\n",
      "Episode is 147, episod_reward is 10.0\n",
      "Episode is 148, episod_reward is 38.0\n",
      "Episode is 149, episod_reward is 31.0\n",
      "Episode is 150, episod_reward is 13.0\n",
      "Episode is 151, episod_reward is 10.0\n",
      "Episode is 152, episod_reward is 29.0\n",
      "Episode is 153, episod_reward is 21.0\n",
      "Episode is 154, episod_reward is 18.0\n",
      "Episode is 155, episod_reward is 23.0\n",
      "Episode is 156, episod_reward is 32.0\n",
      "Episode is 157, episod_reward is 12.0\n",
      "Episode is 158, episod_reward is 14.0\n",
      "Episode is 159, episod_reward is 30.0\n",
      "Episode is 160, episod_reward is 12.0\n",
      "Episode is 161, episod_reward is 24.0\n",
      "Episode is 162, episod_reward is 21.0\n",
      "Episode is 163, episod_reward is 28.0\n",
      "Episode is 164, episod_reward is 21.0\n",
      "Episode is 165, episod_reward is 12.0\n",
      "Episode is 166, episod_reward is 33.0\n",
      "Episode is 167, episod_reward is 10.0\n",
      "Episode is 168, episod_reward is 21.0\n",
      "Episode is 169, episod_reward is 38.0\n",
      "Episode is 170, episod_reward is 12.0\n",
      "Episode is 171, episod_reward is 23.0\n",
      "Episode is 172, episod_reward is 15.0\n",
      "Episode is 173, episod_reward is 15.0\n",
      "Episode is 174, episod_reward is 17.0\n",
      "Episode is 175, episod_reward is 22.0\n",
      "Episode is 176, episod_reward is 27.0\n",
      "Episode is 177, episod_reward is 14.0\n",
      "Episode is 178, episod_reward is 14.0\n",
      "Episode is 179, episod_reward is 20.0\n",
      "Episode is 180, episod_reward is 24.0\n",
      "Episode is 181, episod_reward is 24.0\n",
      "Episode is 182, episod_reward is 46.0\n",
      "Episode is 183, episod_reward is 19.0\n",
      "Episode is 184, episod_reward is 22.0\n",
      "Episode is 185, episod_reward is 38.0\n",
      "Episode is 186, episod_reward is 12.0\n",
      "Episode is 187, episod_reward is 57.0\n",
      "Episode is 188, episod_reward is 30.0\n",
      "Episode is 189, episod_reward is 16.0\n",
      "Episode is 190, episod_reward is 12.0\n",
      "Episode is 191, episod_reward is 24.0\n",
      "Episode is 192, episod_reward is 21.0\n",
      "Episode is 193, episod_reward is 20.0\n",
      "Episode is 194, episod_reward is 8.0\n",
      "Episode is 195, episod_reward is 52.0\n",
      "Episode is 196, episod_reward is 14.0\n",
      "Episode is 197, episod_reward is 39.0\n",
      "Episode is 198, episod_reward is 21.0\n",
      "Episode is 199, episod_reward is 16.0\n",
      "Episode is 200, episod_reward is 13.0\n",
      "Episode is 201, episod_reward is 31.0\n",
      "Episode is 202, episod_reward is 26.0\n",
      "Episode is 203, episod_reward is 28.0\n",
      "Episode is 204, episod_reward is 16.0\n",
      "Episode is 205, episod_reward is 43.0\n",
      "Episode is 206, episod_reward is 17.0\n",
      "Episode is 207, episod_reward is 23.0\n",
      "Episode is 208, episod_reward is 17.0\n",
      "Episode is 209, episod_reward is 34.0\n",
      "Episode is 210, episod_reward is 28.0\n",
      "Episode is 211, episod_reward is 9.0\n",
      "Episode is 212, episod_reward is 17.0\n",
      "Episode is 213, episod_reward is 20.0\n",
      "Episode is 214, episod_reward is 23.0\n",
      "Episode is 215, episod_reward is 10.0\n",
      "Episode is 216, episod_reward is 23.0\n",
      "Episode is 217, episod_reward is 13.0\n",
      "Episode is 218, episod_reward is 13.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode is 219, episod_reward is 12.0\n",
      "Episode is 220, episod_reward is 11.0\n",
      "Episode is 221, episod_reward is 23.0\n",
      "Episode is 222, episod_reward is 18.0\n",
      "Episode is 223, episod_reward is 16.0\n",
      "Episode is 224, episod_reward is 15.0\n",
      "Episode is 225, episod_reward is 20.0\n",
      "Episode is 226, episod_reward is 24.0\n",
      "Episode is 227, episod_reward is 21.0\n",
      "Episode is 228, episod_reward is 13.0\n",
      "Episode is 229, episod_reward is 11.0\n",
      "Episode is 230, episod_reward is 13.0\n",
      "Episode is 231, episod_reward is 19.0\n",
      "Episode is 232, episod_reward is 17.0\n",
      "Episode is 233, episod_reward is 20.0\n",
      "Episode is 234, episod_reward is 15.0\n",
      "Episode is 235, episod_reward is 35.0\n",
      "Episode is 236, episod_reward is 47.0\n",
      "Episode is 237, episod_reward is 9.0\n",
      "Episode is 238, episod_reward is 27.0\n",
      "Episode is 239, episod_reward is 17.0\n",
      "Episode is 240, episod_reward is 31.0\n",
      "Episode is 241, episod_reward is 24.0\n",
      "Episode is 242, episod_reward is 15.0\n",
      "Episode is 243, episod_reward is 28.0\n",
      "Episode is 244, episod_reward is 14.0\n",
      "Episode is 245, episod_reward is 29.0\n",
      "Episode is 246, episod_reward is 12.0\n",
      "Episode is 247, episod_reward is 44.0\n",
      "Episode is 248, episod_reward is 22.0\n",
      "Episode is 249, episod_reward is 15.0\n",
      "Episode is 250, episod_reward is 23.0\n",
      "Episode is 251, episod_reward is 23.0\n",
      "Episode is 252, episod_reward is 17.0\n",
      "Episode is 253, episod_reward is 22.0\n",
      "Episode is 254, episod_reward is 13.0\n",
      "Episode is 255, episod_reward is 11.0\n",
      "Episode is 256, episod_reward is 36.0\n",
      "Episode is 257, episod_reward is 19.0\n",
      "Episode is 258, episod_reward is 27.0\n",
      "Episode is 259, episod_reward is 20.0\n",
      "Episode is 260, episod_reward is 22.0\n",
      "Episode is 261, episod_reward is 13.0\n",
      "Episode is 262, episod_reward is 18.0\n",
      "Episode is 263, episod_reward is 33.0\n",
      "Episode is 264, episod_reward is 20.0\n",
      "Episode is 265, episod_reward is 30.0\n",
      "Episode is 266, episod_reward is 43.0\n",
      "Episode is 267, episod_reward is 22.0\n",
      "Episode is 268, episod_reward is 15.0\n",
      "Episode is 269, episod_reward is 29.0\n",
      "Episode is 270, episod_reward is 16.0\n",
      "Episode is 271, episod_reward is 15.0\n",
      "Episode is 272, episod_reward is 27.0\n",
      "Episode is 273, episod_reward is 30.0\n",
      "Episode is 274, episod_reward is 24.0\n",
      "Episode is 275, episod_reward is 14.0\n",
      "Episode is 276, episod_reward is 17.0\n",
      "Episode is 277, episod_reward is 11.0\n",
      "Episode is 278, episod_reward is 11.0\n",
      "Episode is 279, episod_reward is 9.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15632/3084108965.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mepisode_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 一共是 5000轮，每轮最多1000步\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15632/2793345638.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(env, agent, max_episode, max_steps, batch_size, render)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m                 \u001b[0mwriter_scale\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss/q1_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummuries\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'q1_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[0mwriter_scale\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss/q2_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummuries\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'q1_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15632/3741044753.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mq1_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[0mq2_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq1_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq2_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\torch\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\torch\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\torch\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    105\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             F.adam(params_with_grad,\n\u001b[0m\u001b[0;32m    108\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\torch\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "episode_rewards = train(env, agent, 10000, 200, 64, render=True) # 一共是 5000轮，每轮最多1000步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a16043",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9781d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net,'policy_2018.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41af198",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ecbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy_net = torch.load('policy_2018.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5dab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(100):\n",
    "        obs = env.reset()\n",
    "        for j in range(20000):\n",
    "            env.render()\n",
    "            action = agent.test_get_action(obs)\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "            obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02904720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

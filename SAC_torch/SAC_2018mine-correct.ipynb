{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "188955bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque  # 双向队列\n",
    "import torch.optim as optim\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d32b92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6dea22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_policy_graph = SummaryWriter('sac_2018_actions/policy_net')\n",
    "writer_softq_graph = SummaryWriter('sac_2018_actions/SoftQ_net')\n",
    "writer_value_graph = SummaryWriter('sac_2018_actions/value_net')\n",
    "writer_scale = SummaryWriter('sac_2018_actions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36d1c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.l1 = nn.Linear(input_dim, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, output_dim)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.l1(state))\n",
    "        x = torch.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "\n",
    "class SoftQNetwork(nn.Module):\n",
    "    '''Critic，多输入网络'''\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=256):\n",
    "        super(SoftQNetwork, self).__init__()\n",
    "        self.l1 = nn.Linear(num_inputs + num_actions, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.l3 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)  # 首先把两个输入合并一起\n",
    "        x = torch.relu(self.l1(x))\n",
    "        x = torch.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "    \n",
    "class PolicyNetwork(nn.Module):\n",
    "    '''Actor，多输出网络'''\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size=256, log_std_min=-20, log_std_max=2):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.log_std_min = log_std_min                              # 这两个咋来的\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.l1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.l_mean = nn.Linear(hidden_size, num_actions)\n",
    "        self.l_logstd = nn.Linear(hidden_size, num_actions)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        '''根据给定的state得到，mean和log_std'''\n",
    "        x = torch.relu(self.l1(state))\n",
    "        x = torch.relu(self.l2(x))\n",
    "        \n",
    "        mean = self.l_mean(x)\n",
    "        log_std = self.l_logstd(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)  # torch的clip其实是clamp\n",
    "\n",
    "        return mean, log_std\n",
    "    \n",
    "    def sample(self, state, epsilon=1e-6):\n",
    "        '''根据state得到mean和logstd之后再计算action(-1,1)和log_pi，是式子26'''\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        normal = Normal(mean, std)\n",
    "        u = normal.rsample()        # rsample()  :从normal高斯分布采样一个u, 形状和mean相同\n",
    "        action = torch.tanh(u)      # action取tanh u\n",
    "        \n",
    "        log_pi = normal.log_prob(u) - torch.log(1 - action.pow(2) + epsilon) # (26)\n",
    "        log_pi = log_pi.sum(1, keepdim=True)                                # 用.sum(dim, keep_dim)这个语法\n",
    "        \n",
    "        return action, log_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17201792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        experience = (state, action, np.array([reward]), next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        done_batch = []\n",
    "        \n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        for experience in batch:\n",
    "            state, action, reward, next_state, done = experience\n",
    "            state_batch.append(state)\n",
    "            action_batch.append(action)\n",
    "            reward_batch.append(reward)\n",
    "            next_state_batch.append(next_state)\n",
    "            done_batch.append(done)\n",
    "            \n",
    "        return (state_batch, action_batch, reward_batch, next_state_batch, done_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "136cb519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(model, target_model, tau):\n",
    "    for target_pam, pam in zip(target_model.parameters(), model.parameters()):\n",
    "        target_pam.data.copy_((1. - tau) * target_pam + tau * pam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65d3e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC_Agent:\n",
    "    def __init__(self, env, gamma, tau, v_lr, q_lr, policy_lr, buffer_maxlen):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        self.env = env\n",
    "        self.action_range = [self.env.action_space.low, self.env.action_space.high]\n",
    "        self.obs_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.update_step = 0\n",
    "        self.delay_step = 2\n",
    "        \n",
    "        ## 初始化网络\n",
    "        self.policy_net = PolicyNetwork(self.obs_dim, self.action_dim).to(self.device)\n",
    "        self.q_net1 = SoftQNetwork(self.obs_dim, self.action_dim).to(self.device)\n",
    "        self.q_net2 = SoftQNetwork(self.obs_dim, self.action_dim).to(self.device)\n",
    "        self.value_net = ValueNetwork(self.obs_dim, 1).to(self.device)\n",
    "        self.target_value_net =  ValueNetwork(self.obs_dim, 1).to(self.device)\n",
    "        \n",
    "        # 初始化目标网络的权重\n",
    "        update_target(self.value_net, self.target_value_net, tau=1.)\n",
    "        \n",
    "        ## 优化器\n",
    "        self.q1_optimizer = optim.Adam(self.q_net1.parameters(), lr=q_lr)\n",
    "        self.q2_optimizer = optim.Adam(self.q_net2.parameters(), lr=q_lr)\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=policy_lr)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=v_lr)\n",
    "        \n",
    "        ## Q网络和V网络的损失函数，都是mse\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        \n",
    "        # 经验池\n",
    "        self.replay_buffer = BasicBuffer(buffer_maxlen)\n",
    "        \n",
    "        # 用来记录各种标量的字典\n",
    "        self.summuries = {}\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        # FloatTensor是把列表或者数组直接转换成tensorfloat32的函数，unsqueeze(dim)是在指定位置插入新维度\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)  # 输入变量也都 to device\n",
    "        mean, log_std = self.policy_net(state)  # 自动call forward\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        normal = Normal(mean, std)\n",
    "        u = normal.sample()     # sample() 和 rsample() 有所不同，这个一般用于不参与梯度计算的时候\n",
    "        \n",
    "        action = torch.tanh(u)\n",
    "        \n",
    "        action = action.cpu().detach().squeeze(0).numpy()  # 需要转移到cpu上，不然显示不了这个值\n",
    "        scaled_action = self.rescale_action(action)\n",
    "        return scaled_action  # (-2,2)\n",
    "    \n",
    "    def test_get_action(self, state):\n",
    "        # 去掉方差， 测试的时候，只用均值即可\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)  # 输入变量也都 to device\n",
    "        mean, _ = self.policy_net(state)  # 自动call forward\n",
    "     \n",
    "        action = torch.tanh(mean)\n",
    "        \n",
    "        action = action.cpu().detach().squeeze(0).numpy()  # 需要转移到cpu上，不然显示不了这个值\n",
    "        scaled_action = self.rescale_action(action)\n",
    "        return scaled_action  # (-2,2)\n",
    "        \n",
    "    \n",
    "    def rescale_action(self, action):\n",
    "        '''经过tanh，action必在[-1,1]，所以需要rescale到动作区间'''\n",
    "        action = action * (self.action_range[1]-self.action_range[0]) / 2.0 +\\\n",
    "                (self.action_range[1] + self.action_range[0]) / 2.0\n",
    "        return action\n",
    "    \n",
    "    def update(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.FloatTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)  # 单个的Bool值不能转换，但是列表可以\n",
    "        dones = dones.view(dones.size(0), -1)  # (batch_size, 1) 的形状\n",
    "        \n",
    "        # 计算q网络相关的值，用的actions都是 (-2,2)之间的动作值\n",
    "        v_actions, v_log_pi = self.policy_net.sample(states)\n",
    "        v_q1 = self.q_net1(states, v_actions)\n",
    "        v_q2 = self.q_net2(states, v_actions)\n",
    "        next_v = self.target_value_net(next_states)\n",
    "        \n",
    "        # value loss \n",
    "        v_target = torch.min(v_q1, v_q2) - v_log_pi\n",
    "        curr_v = self.value_net(states)\n",
    "        v_loss = self.loss_fn(curr_v, v_target.detach()) # 目标值统统detach中断传播\n",
    "        \n",
    "        # q_loss and update_qnet\n",
    "        curr_q1 = self.q_net1(states, actions)\n",
    "        curr_q2 = self.q_net2(states, actions)\n",
    "        expected_q = rewards + (1 - dones) * self.gamma * next_v\n",
    "        q1_loss = self.loss_fn(curr_q1, expected_q.detach()) # 目标值不需要梯度计算，所以detach终止梯度\n",
    "        q2_loss = self.loss_fn(curr_q2, expected_q.detach())\n",
    "        self.summuries['q1_loss'] = q1_loss.detach().item()\n",
    "        self.summuries['q2_loss'] = q2_loss.detach().item()\n",
    "        \n",
    "        self.value_optimizer.zero_grad()\n",
    "        self.q1_optimizer.zero_grad()\n",
    "        self.q2_optimizer.zero_grad()\n",
    "        v_loss.backward()\n",
    "        q1_loss.backward()\n",
    "        q2_loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "        self.q1_optimizer.step()\n",
    "        self.q2_optimizer.step()\n",
    "        \n",
    "        # 延迟更新policy网络以及q目标网络，用的actions 是 (-1,1)之间的动作值\n",
    "        new_actions, log_pi = self.policy_net.sample(states)  # new_actions 是-1,1之间的值\n",
    "        if self.update_step % self.delay_step == 0:\n",
    "            # 更新 policy网络\n",
    "            min_q = torch.min(self.q_net1(states, new_actions), \n",
    "                              self.q_net2(states, new_actions))\n",
    "            \n",
    "            policy_loss = (log_pi - min_q).mean()\n",
    "            self.summuries['policy_loss'] = policy_loss.detach().item()\n",
    "            \n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.policy_optimizer.step()\n",
    "            \n",
    "            # 更新目标v网络\n",
    "            update_target(self.value_net, self.target_value_net, tau=self.tau)\n",
    "        \n",
    "        self.update_step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1701e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, max_episode, max_steps, batch_size, render=True):\n",
    "    global_step = 0\n",
    "    \n",
    "    for episode in range(max_episode):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_step = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            if render:\n",
    "                env.render()\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "            episode_reward += reward  # 每个回合能获得多少奖励\n",
    "            global_step += 1          # 全部步数\n",
    "            episode_step += 1         # 每个回合能走几步\n",
    "            writer_scale.add_scalar('Main/every_step_reward', reward, global_step) # 每一小步的单步奖励\n",
    "            \n",
    "            if len(agent.replay_buffer.buffer) > batch_size:\n",
    "                agent.update(batch_size)\n",
    "                writer_scale.add_scalar('Loss/q1_loss', agent.summuries['q1_loss'], global_step)\n",
    "                writer_scale.add_scalar('Loss/q2_loss', agent.summuries['q1_loss'], global_step)\n",
    "                writer_scale.add_scalar('Loss/policy_loss',agent.summuries['policy_loss'], global_step)\n",
    "\n",
    "            if done or step == max_steps - 1:\n",
    "                writer_scale.add_scalar('Episode/episode_steps', episode_step, episode)\n",
    "                writer_scale.add_scalar('Episode/episode_rewards', episode_reward, episode)\n",
    "                print('Episode is {}, episod_reward is {}'.format(episode, episode_reward))\n",
    "                break\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0fc6d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('Ant-v2')\n",
    "env = gym.make('Hopper-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff6d238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAC 2018 Params\n",
    "tau = 0.005\n",
    "gamma = 0.99\n",
    "value_lr = 3e-4\n",
    "q_lr = 3e-4\n",
    "policy_lr = 3e-4\n",
    "buffer_maxlen = 1000000\n",
    "\n",
    "#2018 agent\n",
    "agent = SAC_Agent(env, gamma, tau, value_lr, q_lr, policy_lr, buffer_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "125f4efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_state_raw = env.reset()\n",
    "test_state = torch.FloatTensor(test_state_raw).unsqueeze(0).to(agent.device)\n",
    "test_action_raw = agent.get_action(test_state_raw)\n",
    "test_action = torch.FloatTensor(test_action_raw).unsqueeze(0).to(agent.device)\n",
    "\n",
    "writer_policy_graph.add_graph(agent.policy_net, test_state)\n",
    "writer_softq_graph.add_graph(agent.q_net1, [test_state, test_action])\n",
    "writer_value_graph.add_graph(agent.value_net, test_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3acef0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode is 0, episod_reward is 15.972597718270409\n",
      "Episode is 1, episod_reward is 9.31604829715257\n",
      "Episode is 2, episod_reward is 8.997190342042845\n",
      "Episode is 3, episod_reward is 13.209068597647383\n",
      "Episode is 4, episod_reward is 9.333430011332478\n",
      "Episode is 5, episod_reward is 15.11782372748069\n",
      "Episode is 6, episod_reward is 23.455744412772095\n",
      "Episode is 7, episod_reward is 12.963882502185637\n",
      "Episode is 8, episod_reward is 28.94919425077684\n",
      "Episode is 9, episod_reward is 28.643390633177546\n",
      "Episode is 10, episod_reward is 6.147201728951146\n",
      "Episode is 11, episod_reward is 15.942191397993835\n",
      "Episode is 12, episod_reward is 8.396573496684072\n",
      "Episode is 13, episod_reward is 15.928785080465712\n",
      "Episode is 14, episod_reward is 11.450712678279528\n",
      "Episode is 15, episod_reward is 7.691701262515537\n",
      "Episode is 16, episod_reward is 9.013170857243237\n",
      "Episode is 17, episod_reward is 5.729856852665965\n",
      "Episode is 18, episod_reward is 9.004171638780738\n",
      "Episode is 19, episod_reward is 33.52709828259998\n",
      "Episode is 20, episod_reward is 8.107770912854287\n",
      "Episode is 21, episod_reward is 7.52395994295419\n",
      "Episode is 22, episod_reward is 9.355702590522473\n",
      "Episode is 23, episod_reward is 36.04915449067558\n",
      "Episode is 24, episod_reward is 7.6689646961093265\n",
      "Episode is 25, episod_reward is 16.42821535745427\n",
      "Episode is 26, episod_reward is 57.355596168000126\n",
      "Episode is 27, episod_reward is 11.696187082742009\n",
      "Episode is 28, episod_reward is 15.763735572908079\n",
      "Episode is 29, episod_reward is 9.809048256337906\n",
      "Episode is 30, episod_reward is 7.595537336007033\n",
      "Episode is 31, episod_reward is 28.403604465425907\n",
      "Episode is 32, episod_reward is 9.38884641682686\n",
      "Episode is 33, episod_reward is 12.462439253598966\n",
      "Episode is 34, episod_reward is 17.495959257043573\n",
      "Episode is 35, episod_reward is 59.06500491471069\n",
      "Episode is 36, episod_reward is 8.868484489461437\n",
      "Episode is 37, episod_reward is 12.796870350746842\n",
      "Episode is 38, episod_reward is 15.973262463676845\n",
      "Episode is 39, episod_reward is 191.4471489676802\n",
      "Episode is 40, episod_reward is 67.93239307959847\n",
      "Episode is 41, episod_reward is 9.111663017284771\n",
      "Episode is 42, episod_reward is 88.25035513283629\n",
      "Episode is 43, episod_reward is 6.729808119982663\n",
      "Episode is 44, episod_reward is 113.81716397786303\n",
      "Episode is 45, episod_reward is 33.45934151159825\n",
      "Episode is 46, episod_reward is 92.87050086441076\n",
      "Episode is 47, episod_reward is 101.13166521410538\n",
      "Episode is 48, episod_reward is 32.36194872040158\n",
      "Episode is 49, episod_reward is 26.130773272375198\n",
      "Episode is 50, episod_reward is 14.130722275552651\n",
      "Episode is 51, episod_reward is 16.01921450384178\n",
      "Episode is 52, episod_reward is 25.640730740001565\n",
      "Episode is 53, episod_reward is 83.14952311371941\n",
      "Episode is 54, episod_reward is 9.504851339013198\n",
      "Episode is 55, episod_reward is 26.5646064198011\n",
      "Episode is 56, episod_reward is 91.41146214073757\n",
      "Episode is 57, episod_reward is 17.962347539563154\n",
      "Episode is 58, episod_reward is 109.08157524930449\n",
      "Episode is 59, episod_reward is 30.57031929999054\n",
      "Episode is 60, episod_reward is 50.406818175537325\n",
      "Episode is 61, episod_reward is 12.74642816805831\n",
      "Episode is 62, episod_reward is 40.52335388078763\n",
      "Episode is 63, episod_reward is 61.36829314899899\n",
      "Episode is 64, episod_reward is 22.84243627193219\n",
      "Episode is 65, episod_reward is 151.60197401429588\n",
      "Episode is 66, episod_reward is 43.677290881450496\n",
      "Episode is 67, episod_reward is 91.92236431794994\n",
      "Episode is 68, episod_reward is 76.18052637234211\n",
      "Episode is 69, episod_reward is 181.73702570815368\n",
      "Episode is 70, episod_reward is 90.09721610569694\n",
      "Episode is 71, episod_reward is 32.474079276748746\n",
      "Episode is 72, episod_reward is 60.38784215260649\n",
      "Episode is 73, episod_reward is 82.87284052924836\n",
      "Episode is 74, episod_reward is 70.20721984359805\n",
      "Episode is 75, episod_reward is 106.30654793886406\n",
      "Episode is 76, episod_reward is 111.00414852396209\n",
      "Episode is 77, episod_reward is 98.09676809949369\n",
      "Episode is 78, episod_reward is 82.55183480330643\n",
      "Episode is 79, episod_reward is 113.94695897330281\n",
      "Episode is 80, episod_reward is 79.8800880916698\n",
      "Episode is 81, episod_reward is 65.73523986576166\n",
      "Episode is 82, episod_reward is 190.09827873791153\n",
      "Episode is 83, episod_reward is 101.55486267670223\n",
      "Episode is 84, episod_reward is 114.58273939978307\n",
      "Episode is 85, episod_reward is 92.40157198861272\n",
      "Episode is 86, episod_reward is 304.67471188269496\n",
      "Episode is 87, episod_reward is 107.89765623567688\n",
      "Episode is 88, episod_reward is 255.8837241014345\n",
      "Episode is 89, episod_reward is 88.7166757001384\n",
      "Episode is 90, episod_reward is 201.94469107582748\n",
      "Episode is 91, episod_reward is 170.85879453619398\n",
      "Episode is 92, episod_reward is 97.98590942034998\n",
      "Episode is 93, episod_reward is 150.07302528053856\n",
      "Episode is 94, episod_reward is 178.10085281595252\n",
      "Episode is 95, episod_reward is 236.05744056498554\n",
      "Episode is 96, episod_reward is 246.26577199481045\n",
      "Episode is 97, episod_reward is 105.69387424442331\n",
      "Episode is 98, episod_reward is 316.2541448088269\n",
      "Episode is 99, episod_reward is 260.98238629835953\n",
      "Episode is 100, episod_reward is 242.79160882777214\n",
      "Episode is 101, episod_reward is 266.6688642078822\n",
      "Episode is 102, episod_reward is 272.78805207598987\n",
      "Episode is 103, episod_reward is 272.22675807363294\n",
      "Episode is 104, episod_reward is 242.76556994154282\n",
      "Episode is 105, episod_reward is 93.21957391192029\n",
      "Episode is 106, episod_reward is 252.50073253126249\n",
      "Episode is 107, episod_reward is 198.04176102669416\n",
      "Episode is 108, episod_reward is 242.4047001966818\n",
      "Episode is 109, episod_reward is 71.10969502435712\n",
      "Episode is 110, episod_reward is 185.84313130370413\n",
      "Episode is 111, episod_reward is 266.14864915105807\n",
      "Episode is 112, episod_reward is 225.3599209244016\n",
      "Episode is 113, episod_reward is 309.11155500987\n",
      "Episode is 114, episod_reward is 211.6094730006547\n",
      "Episode is 115, episod_reward is 191.53907803785893\n",
      "Episode is 116, episod_reward is 248.00899853576473\n",
      "Episode is 117, episod_reward is 245.48045395051093\n",
      "Episode is 118, episod_reward is 189.61925008106795\n",
      "Episode is 119, episod_reward is 186.10775681610397\n",
      "Episode is 120, episod_reward is 191.62215779930847\n",
      "Episode is 121, episod_reward is 101.00714237600356\n",
      "Episode is 122, episod_reward is 200.6143203788183\n",
      "Episode is 123, episod_reward is 194.2311366748042\n",
      "Episode is 124, episod_reward is 218.82525104022423\n",
      "Episode is 125, episod_reward is 224.001526215132\n",
      "Episode is 126, episod_reward is 129.5169190095133\n",
      "Episode is 127, episod_reward is 195.3655516751676\n",
      "Episode is 128, episod_reward is 208.746796295273\n",
      "Episode is 129, episod_reward is 236.68530188865654\n",
      "Episode is 130, episod_reward is 206.28846245701257\n",
      "Episode is 131, episod_reward is 218.5168140531264\n",
      "Episode is 132, episod_reward is 208.92552218476223\n",
      "Episode is 133, episod_reward is 196.55561227557408\n",
      "Episode is 134, episod_reward is 234.38845199007662\n",
      "Episode is 135, episod_reward is 246.88942350892063\n",
      "Episode is 136, episod_reward is 183.61050361071585\n",
      "Episode is 137, episod_reward is 281.7259511474108\n",
      "Episode is 138, episod_reward is 197.8532770871869\n",
      "Episode is 139, episod_reward is 206.77845028407893\n",
      "Episode is 140, episod_reward is 266.1001497308942\n",
      "Episode is 141, episod_reward is 238.04645066179873\n",
      "Episode is 142, episod_reward is 265.0453011229607\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9036/729772296.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mepisode_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 一共是 5000轮，每轮最多1000步\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9036/2793345638.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(env, agent, max_episode, max_steps, batch_size, render)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m                 \u001b[0mwriter_scale\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss/q1_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummuries\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'q1_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[0mwriter_scale\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss/q2_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummuries\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'q1_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9036/3260376817.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq2_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mv_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0mq1_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m         \u001b[0mq2_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\torch\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "episode_rewards = train(env, agent, 10000, 1000, 64, render=False) # 一共是 5000轮，每轮最多1000步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70a16043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1., -1., -1.], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9781d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net,'policy_2018.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41af198",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc3ecbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy_net = torch.load('policy_2018.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac5dab13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating window glfw\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5500/462402589.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_get_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mnext_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\torch\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"human\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\torch\\lib\\site-packages\\gym\\envs\\mujoco\\mujoco_env.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, width, height, camera_id, camera_name)\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"human\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_viewer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\torch\\lib\\site-packages\\mujoco_py\\mjviewer.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loop_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loop_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m                 \u001b[0mrender_inner_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_loop_count\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;31m# Markers and overlay are regenerated in every pass.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\torch\\lib\\site-packages\\mujoco_py\\mjviewer.py\u001b[0m in \u001b[0;36mrender_inner_loop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    176\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_overlay\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_full_overlay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m             \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_video\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m                 \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_pixels_as_in_window\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\torch\\lib\\site-packages\\mujoco_py\\mjviewer.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gui_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mglfw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll_events\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmjrendercontext.pyx\u001b[0m in \u001b[0;36mmujoco_py.cymj.MjRenderContextWindow.render\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\torch\\lib\\site-packages\\glfw\\__init__.py\u001b[0m in \u001b[0;36mswap_buffers\u001b[1;34m(window)\u001b[0m\n\u001b[0;32m   2285\u001b[0m         \u001b[0mvoid\u001b[0m \u001b[0mglfwSwapBuffers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGLFWwindow\u001b[0m\u001b[1;33m*\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2286\u001b[0m     \"\"\"\n\u001b[1;32m-> 2287\u001b[1;33m     \u001b[0m_glfw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglfwSwapBuffers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2289\u001b[0m \u001b[0m_glfw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglfwSwapInterval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\envs\\torch\\lib\\site-packages\\glfw\\__init__.py\u001b[0m in \u001b[0;36merrcheck\u001b[1;34m(result, *args)\u001b[0m\n\u001b[0;32m    625\u001b[0m     \u001b[0musing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0m_callback_exception_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m     \"\"\"\n\u001b[1;32m--> 627\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0merrcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m         \u001b[1;32mglobal\u001b[0m \u001b[0m_exc_info_from_callback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_exc_info_from_callback\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(100):\n",
    "        obs = env.reset()\n",
    "        for j in range(20000):\n",
    "            env.render()\n",
    "            action = agent.test_get_action(obs)\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "            obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02904720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
